{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(ADD THE NOTEBOOK NAME HERE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write your notebook objective here, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write down which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\vscode-projects\\\\CancelScope-Hotel-Booking-Analysis\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\vscode-projects\\\\CancelScope-Hotel-Booking-Analysis'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd # For data manipulation\n",
        "import seaborn as sns # For data visualization\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "from sklearn.model_selection import train_test_split # For splitting the dataset\n",
        "from sklearn.ensemble import RandomForestClassifier # For Random Forest model\n",
        "from sklearn.metrics import classification_report, confusion_matrix # For evaluation metrics\n",
        "from sklearn.metrics import roc_auc_score,precision_score, recall_score # For evaluation metrics\n",
        "from sklearn.metrics import f1_score, precision_recall_curve # For evaluation metrics\n",
        "from sklearn.model_selection import cross_val_score # For cross-validation\n",
        "import joblib # For saving and loading models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = os.path.join(current_dir, \"data\", \"clean\", \"hotel_bookings_sample.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preliminary checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the shape of the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Load the CSV file into a DataFrame\n",
        "# df = pd.read_csv(csv_path)\n",
        "# df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm balans of target label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_canceled\n",
              "True     5000\n",
              "False    5000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['is_canceled'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Missing values count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hotel                            0\n",
              "lead_time                        0\n",
              "arrival_date_year                0\n",
              "arrival_date_month               0\n",
              "weekend_nights                   0\n",
              "week_nights                      0\n",
              "adults                           0\n",
              "children                         0\n",
              "babies                           0\n",
              "is_international                 0\n",
              "days_waiting                     0\n",
              "is_repeated_guest                0\n",
              "is_family                        0\n",
              "is_canceled                      0\n",
              "market_segment_complementary     0\n",
              "market_segment_corporate         0\n",
              "market_segment_direct            0\n",
              "market_segment_groups            0\n",
              "market_segment_offline_ta_to     0\n",
              "market_segment_online_ta         0\n",
              "distribution_channel_direct      0\n",
              "distribution_channel_gds         0\n",
              "distribution_channel_ta_to       0\n",
              "deposit_type_non_refund          0\n",
              "deposit_type_refundable          0\n",
              "customer_type_group              0\n",
              "customer_type_transient          0\n",
              "customer_type_transient_party    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split features/target, train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (8000, 27)\n",
            "Test set shape: (2000, 27)\n"
          ]
        }
      ],
      "source": [
        "# Features and target\n",
        "X = df.drop(columns=['is_canceled'])\n",
        "y = df['is_canceled']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "X_test.to_csv(\"X_test.csv\", index=False)\n",
        "y_test.to_csv(\"y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_test_loaded' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# !!!!\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# For y_test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m y_test_loaded = \u001b[43my_test_loaded\u001b[49m.squeeze()  \u001b[38;5;66;03m# make it a Series\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# For X_test\u001b[39;00m\n\u001b[32m      6\u001b[39m X_test_loaded = X_test_loaded.astype(X_test.dtypes.to_dict())  \u001b[38;5;66;03m# align dtypes\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'y_test_loaded' is not defined"
          ]
        }
      ],
      "source": [
        "# !!!!\n",
        "# For y_test\n",
        "y_test_loaded = y_test_loaded.squeeze()  # make it a Series\n",
        "\n",
        "# For X_test\n",
        "X_test_loaded = X_test_loaded.astype(X_test.dtypes.to_dict())  # align dtypes\n",
        "\n",
        "# Reset indices just to be safe\n",
        "X_test_loaded = X_test_loaded.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test_loaded = y_test_loaded.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "# Compare\n",
        "print(\"X_test equal:\", X_test.equals(X_test_loaded))\n",
        "print(\"y_test equal:\", y_test.equals(y_test_loaded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# Feature importance\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "feature_importances.head(10).plot(kind='barh', title='Top 10 Feature Importances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Predicted probabilities for the positive class (cancellations)\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Thresholds to evaluate\n",
        "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "print(\"Threshold | Precision | Recall | F1-score\")\n",
        "print(\"----------------------------------------\")\n",
        "for t in thresholds:\n",
        "    y_pred_t = (y_proba >= t)\n",
        "    precision = precision_score(y_test, y_pred_t)\n",
        "    recall = recall_score(y_test, y_pred_t)\n",
        "    f1 = f1_score(y_test, y_pred_t)\n",
        "    print(f\"{t:.1f}      | {precision:.2f}     | {recall:.2f}   | {f1:.2f}\")\n",
        "\n",
        "# Optional: plot Precision-Recall vs Threshold\n",
        "precision_vals, recall_vals, thresh_vals = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(thresh_vals, precision_vals[:-1], label='Precision')\n",
        "plt.plot(thresh_vals, recall_vals[:-1], label='Recall')\n",
        "plt.xlabel('Probability Threshold')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision and Recall vs Threshold')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal is to catch as many cancelations as possible (high recall), but also I want to avoid too many false positives. \n",
        "That means I need a balanced threshold -low enough to catch most cancellations but not so low to get too many false positives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply threshold\n",
        "threshold = 0.3\n",
        "y_pred_final = (y_proba >= threshold)\n",
        "\n",
        "# Classification report\n",
        "print(f\"Classification report at threshold {threshold}:\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_final)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Canceled','Canceled'], yticklabels=['Not Canceled','Canceled'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix (threshold={threshold})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall for canceled bookings (True): 0.90 → You’re catching 90% of actual cancellations, which is excellent for proactive actions.\n",
        "\n",
        "Precision for canceled bookings: 0.63 → About 37% of bookings predicted as canceled are actually not canceled. So some false positives exist.\n",
        "\n",
        "Recall for non-canceled bookings (False): 0.48 → Less than half of non-canceled bookings are correctly predicted, which is expected because we lowered the threshold to catch more cancellations.\n",
        "\n",
        "Accuracy: 0.69 → Overall correct predictions are 69%. Accuracy is not the main focus here since the classes are balanced in this subset.\n",
        "\n",
        "ROC-AUC: 0.76 → The model still has good discriminative ability.\n",
        "\n",
        "Business Implication:\n",
        "\n",
        "Using threshold 0.3 prioritizes catching cancellations, which is key for actions like sending reminders, offering incentives, or reallocating resources.\n",
        "\n",
        "Some false positives are inevitable, but this is an acceptable trade-off when the goal is high recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicted probabilities for the positive class\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Set new threshold\n",
        "threshold = 0.4\n",
        "y_pred_threshold = (y_proba >= threshold)\n",
        "\n",
        "# Evaluate metrics at threshold 0.4\n",
        "print(f\"Classification report at threshold {threshold}:\")\n",
        "print(classification_report(y_test, y_pred_threshold))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I can conclude that: higher threshold leads to fewer false positives, but more missed cancellations. Lower threshold catches more cancellations, but more false alarms.\n",
        "In the business requirement, cost of missing a cancellation is high (lost revenue, last-minute vacancies) and sending offers/incentives to prevent cancellations is cheap and automated.\n",
        "The business goal is to proactively reach out (via offers or incentives) to customers likely to cancel, and for that reason I will consider a lower threshold as 0,3 valid\n",
        "This approach maximizes the number of potential cancellations cought, even if it means some customers who wouldn’t have canceled receive offers. If the cost of sending offers is low compared to the potential benefit of saving bookings, this is a sound strategy. A further/future analysis would develop this idea and will analyse potential costs of cancelations, offers, automated systems etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_weighted = RandomForestClassifier(\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    n_estimators=500,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    max_depth=20\n",
        ")\n",
        "rf_weighted.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Check feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame of feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf_weighted.feature_importances_\n",
        "})\n",
        "\n",
        "# Sort by importance descending\n",
        "feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
        "\n",
        "# Display top 10 features\n",
        "print(feature_importances.head(10))\n",
        "\n",
        "# Plot top 10 features\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importances.head(10))\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- lead_time (0.25)\tMost predictive feature. Bookings made far in advance are much more likely to cancel — customers have more time to change plans\t\n",
        "- is_international (0.15)\tInternational guests have higher cancellation risk (travel restrictions, visa issues, flight costs)\t\n",
        "- arrival_date_month (0.08)\tSeasonality plays a role — some months (summer, holidays) may have higher cancellation or rescheduling rates\t\n",
        "- market_segment_online_ta (0.08)\tBookings through online travel agencies (OTAs) tend to have more flexible cancellation policies — higher risk\t\n",
        "- week_nights (0.07)\tLonger weekday stays might correlate with business travel, which can fluctuate or get canceled\t\n",
        "- arrival_date_year (0.07)\tSome years might reflect macro patterns (like travel restrictions or demand recovery)\t\n",
        "- market_segment_offline_ta_to (0.04)\tOffline travel agent or tour operator bookings tend to be more stable — lower risk\t\n",
        "- weekend_nights (0.04)\tShorter weekend stays often have less cancellation risk\t\n",
        "- adults (0.03)\tFamily/group bookings are usually more stable than solo travelers\t\n",
        "- hotel (0.03)\tThe type of hotel (city vs resort) still influences cancellation likelihood — resorts often see higher rates due to vacation planning uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model using 5-fold cross-validation on the training data\n",
        "cv_scores = cross_val_score(\n",
        "    rf_weighted, X_train, y_train,\n",
        "    cv=5, scoring='roc_auc', n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Cross-validation ROC-AUC scores:\", cv_scores)\n",
        "print(\"Mean ROC-AUC:\", cv_scores.mean())\n",
        "print(\"Standard deviation:\", cv_scores.std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model’s cross-validated ROC-AUC averages 0.79 with a low standard deviation, demonstrating both strong discriminatory power and stability across different data subsets. This suggests the model is reliable for predicting booking cancellations and is unlikely to be overfitting to any particular fold of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the path to save the model\n",
        "model_path = os.path.join(current_dir, \"model\", \"cancelscope_model.pkl\")\n",
        "\n",
        "# Save the trained model (rf_weighted) to the specified file\n",
        "joblib.dump(rf_weighted, model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create your folder here\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
